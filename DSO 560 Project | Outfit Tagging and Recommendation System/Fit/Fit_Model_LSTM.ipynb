{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shashank-r97/NLP/blob/master/Style/Model_V5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "OZMWoVO4kknG",
    "outputId": "522c994a-f433-4ef8-ea2e-0c9597dd06d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential \n",
    "from keras.layers import Input, Masking\n",
    "from keras.layers import Dense, Input, Reshape\n",
    "from keras.layers import Dropout, Activation\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Activation\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "import keras.backend as K \n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "1J88vxQgkvZY",
    "outputId": "d0ceb7a7-95b1-4779-b85f-3ca4d8697028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.29 s, sys: 79.1 ms, total: 8.37 s\n",
      "Wall time: 8.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read the data\n",
    "df = pd.read_excel('data.xlsx')\n",
    "\n",
    "# creating a subset of the relevant attribute name and then dropping the column\n",
    "df = df[df['attribute_name'] == 'fit'].drop(columns = ['attribute_name'])\n",
    "\n",
    "# combining similar category attribute values and removing spaces\n",
    "df['attribute_value'] = df['attribute_value'].apply(lambda x: 'semifit' if x == 'semi fit' else x)\n",
    "df['attribute_value'] = df['attribute_value'].apply(lambda x: 'fittedtailor' if x == 'fitted tailor' else x)\n",
    "df['attribute_value'] = df['attribute_value'].apply(lambda x: 'semifit' if x == 'semifitte' else x)\n",
    "df['attribute_value'] = df['attribute_value'].apply(lambda x: 'fittedtailor' if x == 'fittedtailore' else x)\n",
    "df['attribute_value'] = df['attribute_value'].apply(lambda x: 'straightregular' if x == 'straight regular' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "vyu18_culshy",
    "outputId": "740878ca-69c1-4f96-d6dd-a4e3cab464da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_full_name</th>\n",
       "      <th>description</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>attribute_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01E223GDRKR84THXZ54GJEW60Y</td>\n",
       "      <td>vince</td>\n",
       "      <td>cashmere camisole</td>\n",
       "      <td>delicate crochet outlines luxe tank pure cashmere</td>\n",
       "      <td>unknown</td>\n",
       "      <td>semifit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>01DPGSTG4M1RXB26QMMN0MPPB8</td>\n",
       "      <td>j crew</td>\n",
       "      <td>mockneck sweater supersoft yarn</td>\n",
       "      <td>cozy flattering mockneck sweater feel good loo...</td>\n",
       "      <td>sweater</td>\n",
       "      <td>semifit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>01DPETFFTSDHQ66H03T0F58NX7</td>\n",
       "      <td>j crew</td>\n",
       "      <td>hanro ultralight spaghetti camisole</td>\n",
       "      <td>supersoft mercerize cotton feel like second sk...</td>\n",
       "      <td>pajama intimate</td>\n",
       "      <td>fittedtailor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>01DVCRRBK29DWT82DRBVVY544X</td>\n",
       "      <td>mother pearl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>black white lyocell partially conceal snap fas...</td>\n",
       "      <td>clothing top blouse</td>\n",
       "      <td>semifit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>01DVCRRBK29DWT82DRBVVY544X</td>\n",
       "      <td>mother pearl</td>\n",
       "      <td>net sustain mile faux pearl embellish print ly...</td>\n",
       "      <td>black white lyocell partially conceal snap fas...</td>\n",
       "      <td>clothing top blouse</td>\n",
       "      <td>semifit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    product_id         brand  \\\n",
       "4   01E223GDRKR84THXZ54GJEW60Y         vince   \n",
       "37  01DPGSTG4M1RXB26QMMN0MPPB8        j crew   \n",
       "48  01DPETFFTSDHQ66H03T0F58NX7        j crew   \n",
       "70  01DVCRRBK29DWT82DRBVVY544X  mother pearl   \n",
       "71  01DVCRRBK29DWT82DRBVVY544X  mother pearl   \n",
       "\n",
       "                                    product_full_name  \\\n",
       "4                                   cashmere camisole   \n",
       "37                    mockneck sweater supersoft yarn   \n",
       "48                hanro ultralight spaghetti camisole   \n",
       "70                                                NaN   \n",
       "71  net sustain mile faux pearl embellish print ly...   \n",
       "\n",
       "                                          description       brand_category  \\\n",
       "4   delicate crochet outlines luxe tank pure cashmere              unknown   \n",
       "37  cozy flattering mockneck sweater feel good loo...              sweater   \n",
       "48  supersoft mercerize cotton feel like second sk...      pajama intimate   \n",
       "70  black white lyocell partially conceal snap fas...  clothing top blouse   \n",
       "71  black white lyocell partially conceal snap fas...  clothing top blouse   \n",
       "\n",
       "   attribute_value  \n",
       "4          semifit  \n",
       "37         semifit  \n",
       "48    fittedtailor  \n",
       "70         semifit  \n",
       "71         semifit  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KCfa3CVn9nY"
   },
   "outputs": [],
   "source": [
    "# creating a feature combining brand, productname, description and brand category\n",
    "\n",
    "df['text'] = (df['brand'] + ' ' + df['product_full_name'] + ' ' + df['description'] + ' ' + df['brand_category']).apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMi2ULgelhFo"
   },
   "outputs": [],
   "source": [
    "# the predictor 'text' is assigned to 'X' and the 'attribute_value' t 'y'\n",
    "X = df['text'].values\n",
    "# one-hot-encoding the y variable\n",
    "y = pd.get_dummies(df['attribute_value']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YFmF01NwQGo"
   },
   "outputs": [],
   "source": [
    "# creating a function to create uni-grams and tokenizing data\n",
    "def encode_1gram(X, mode = 'binary'):\n",
    "    tokenizer = Tokenizer(num_words=500)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    length = max([len(s.split()) for s in df['text']])\n",
    "    X = tokenizer.texts_to_matrix(X)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return X, length, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oETme721eAP"
   },
   "outputs": [],
   "source": [
    "# creating a function to create bi-grams and tokenizing data\n",
    "def encode_2gram(X, mode='binary'):\n",
    "    phrases = Phrases(X, min_count=30)\n",
    "    bigrams = Phraser(phrases)\n",
    "    X = list(bigrams[X])\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=500)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    length = max([len(s.split()) for s in df['text']])\n",
    "    X = tokenizer.texts_to_matrix(X)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return X, length, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "30eS_YrlwyL9",
    "outputId": "b84530f8-b367-4fd0-bd99-47df41a323de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x10a41c6a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6oEaE9mamM7X"
   },
   "outputs": [],
   "source": [
    "# creating tf-idf vectors from uni-gram encoding\n",
    "X1, length1, vocab_size1 = encode_1gram(X, mode = 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGFzNuUUxWmZ"
   },
   "outputs": [],
   "source": [
    "# creating tf-idf vectors from bi-gram encoding\n",
    "X2, length2, vocab_size2 = encode_2gram(X, mode = 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hJpzT_ygA7Pw",
    "outputId": "a7c9e3dd-d50e-4985-f579-81fbcb576a95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3464, 155)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yf8Gl0NGA1HV"
   },
   "outputs": [],
   "source": [
    "# concatenating uni-gram and bi-gram\n",
    "X = np.concatenate([X1, X2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ae88zjZ9CZye"
   },
   "outputs": [],
   "source": [
    "# splitting data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDwRtJkSBvxp"
   },
   "outputs": [],
   "source": [
    "# balancing the sample sets as the proportion of labels is unbalanced\n",
    "resampler = SMOTETomek(sampling_strategy = 'auto')\n",
    "X_train, y_train = resampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNTeR6ZRoxcL"
   },
   "outputs": [],
   "source": [
    "num_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xgZ_gpen5wR"
   },
   "outputs": [],
   "source": [
    "# defining LSTM model with sigmoid activation functions for prediction\n",
    "def define_model_LSTM():\n",
    "    inputs = Input(shape=(length1+length2,))\n",
    "    embedding = Embedding(input_dim=vocab_size1, output_dim=100)(inputs)\n",
    "\n",
    "    x = LSTM(16, return_sequences=True, dropout=0.2, recurrent_dropout=0.15)(embedding)\n",
    "\n",
    "    x = Conv1D(filters=num_classes, kernel_size=1, padding='valid')(x)\n",
    "    x = Conv1D(filters=num_classes, kernel_size=310, padding='valid')(x)\n",
    "    x = Reshape((num_classes,))(x)\n",
    "    out = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs = [inputs], outputs = out)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "WtVkDeU4pOFw",
    "outputId": "fcfe73b5-6956-4e64-c55f-22bfad80a441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 310)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 310, 100)          482700    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 310, 16)           7488      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 310, 5)            85        \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 5)              7755      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 498,028\n",
      "Trainable params: 498,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model_LSTM()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "colab_type": "code",
    "id": "wfeN4H1XpRvM",
    "outputId": "d858c53d-ad4d-403e-94dc-e044d48f9dad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhwanikapadia/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4172 samples, validate on 1044 samples\n",
      "Epoch 1/10\n",
      "4172/4172 [==============================] - 4s 1ms/step - loss: 0.4980 - accuracy: 0.8000 - val_loss: 0.5579 - val_accuracy: 0.8000\n",
      "Epoch 2/10\n",
      "4172/4172 [==============================] - 4s 936us/step - loss: 0.4955 - accuracy: 0.8000 - val_loss: 0.5497 - val_accuracy: 0.8000\n",
      "Epoch 3/10\n",
      "4172/4172 [==============================] - 4s 934us/step - loss: 0.4948 - accuracy: 0.8000 - val_loss: 0.5562 - val_accuracy: 0.8000\n",
      "Epoch 4/10\n",
      "4172/4172 [==============================] - 4s 949us/step - loss: 0.4949 - accuracy: 0.8000 - val_loss: 0.5553 - val_accuracy: 0.8000\n",
      "Epoch 5/10\n",
      "4172/4172 [==============================] - 4s 911us/step - loss: 0.4942 - accuracy: 0.8000 - val_loss: 0.5526 - val_accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "4172/4172 [==============================] - 4s 933us/step - loss: 0.4944 - accuracy: 0.8000 - val_loss: 0.5512 - val_accuracy: 0.8000\n",
      "Epoch 7/10\n",
      "4172/4172 [==============================] - 4s 949us/step - loss: 0.4944 - accuracy: 0.8000 - val_loss: 0.5485 - val_accuracy: 0.8000\n",
      "Epoch 8/10\n",
      "4172/4172 [==============================] - 4s 944us/step - loss: 0.4934 - accuracy: 0.8000 - val_loss: 0.5545 - val_accuracy: 0.8000\n",
      "Epoch 9/10\n",
      "4172/4172 [==============================] - 4s 953us/step - loss: 0.4935 - accuracy: 0.8000 - val_loss: 0.5366 - val_accuracy: 0.8000\n",
      "Epoch 10/10\n",
      "4172/4172 [==============================] - 4s 941us/step - loss: 0.4921 - accuracy: 0.8000 - val_loss: 0.5670 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14c083f98>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting data onto model\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "RH0rgY1WChE4",
    "outputId": "fd677c2f-c95c-4c7e-bed6-3e6222c2a153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/347 [==============================] - 0s 638us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48754795355137215, 0.8000001311302185]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "ythmDKKlpjsc",
    "outputId": "d6f16597-3d88-4156-9dfd-18e674c16b65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2065\n",
       "2    1399\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions for each class from predicting model\n",
    "pd.Series(model.predict(X).argmax(axis = -1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "Xjn1GCho_snJ",
    "outputId": "64e89209-019a-46ea-b9e1-0ddd7a3ce7aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2500319 , 0.15155238, 0.2760512 , 0.1843861 , 0.13797845],\n",
       "       [0.2506047 , 0.14626297, 0.27083907, 0.19791245, 0.13438083],\n",
       "       [0.26671308, 0.1462273 , 0.26894182, 0.18754506, 0.13057283],\n",
       "       ...,\n",
       "       [0.26213434, 0.15416183, 0.2610137 , 0.19468541, 0.12800482],\n",
       "       [0.25776097, 0.14705575, 0.2820309 , 0.19006355, 0.12308883],\n",
       "       [0.22856206, 0.16427822, 0.28661034, 0.18537323, 0.1351762 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of predicting each class for various records\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "5WsQqrmxprsc",
    "outputId": "ae856a95-e7c3-4788-e1e7-d646f27a7852"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1164\n",
       "3     910\n",
       "4     659\n",
       "0     610\n",
       "1     121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split of classes\n",
    "pd.Series(y.argmax(axis = -1)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmfNQQbrqhhc"
   },
   "source": [
    "### LSTM Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text'].values\n",
    "y = pd.get_dummies(df['attribute_value'])\n",
    "label_list = y.columns\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the data and integer encoding it\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the tokenizer function on features\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting vocab size and length for input\n",
    "\n",
    "length = max([len(s.split()) for s in df['text']])\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the data to make it same size (max size)\n",
    "\n",
    "X = pad_sequences(X, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting training and testing data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balancing the dataset to account for disproportionate of labels\n",
    "\n",
    "resampler = SMOTETomek(sampling_strategy = 'auto')\n",
    "X_train, y_train = resampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# using glove vector to create function that makes word emebeddings\n",
    "\n",
    "def load_glove_vectors():\n",
    "    embeddings_index = {}\n",
    "    with open('glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "embeddings_index = load_glove_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an LSTM Glove model\n",
    "\n",
    "def define_model_LSTM_Glove():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=length, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15, activation = 'relu'))\n",
    "    model.add(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.15, activation = 'relu'))\n",
    "    model.add(Dense(16, activation = 'relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 155, 100)          482800    \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 155, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 155, 64)           42240     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 559,189\n",
      "Trainable params: 76,389\n",
      "Non-trainable params: 482,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model_LSTM_Glove()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4073 samples, validate on 1019 samples\n",
      "Epoch 1/10\n",
      "4073/4073 [==============================] - 11s 3ms/step - loss: 0.4967 - accuracy: 0.8000 - val_loss: 0.5205 - val_accuracy: 0.8000\n",
      "Epoch 2/10\n",
      "4073/4073 [==============================] - 10s 2ms/step - loss: 0.4917 - accuracy: 0.7999 - val_loss: 0.5392 - val_accuracy: 0.8000\n",
      "Epoch 3/10\n",
      "4073/4073 [==============================] - 10s 2ms/step - loss: 0.4892 - accuracy: 0.7999 - val_loss: 0.5356 - val_accuracy: 0.8000\n",
      "Epoch 4/10\n",
      "4073/4073 [==============================] - 10s 2ms/step - loss: 0.4862 - accuracy: 0.8000 - val_loss: 0.5402 - val_accuracy: 0.8000\n",
      "Epoch 5/10\n",
      "4073/4073 [==============================] - 10s 2ms/step - loss: 0.4816 - accuracy: 0.8000 - val_loss: 0.5418 - val_accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "4073/4073 [==============================] - 10s 2ms/step - loss: 0.4779 - accuracy: 0.8000 - val_loss: 0.5292 - val_accuracy: 0.8000\n",
      "Epoch 7/10\n",
      "4073/4073 [==============================] - 10s 2ms/step - loss: 0.4690 - accuracy: 0.8006 - val_loss: 0.5322 - val_accuracy: 0.7998\n",
      "Epoch 8/10\n",
      "4073/4073 [==============================] - 10s 2ms/step - loss: 0.4653 - accuracy: 0.8006 - val_loss: 0.5201 - val_accuracy: 0.7994\n",
      "Epoch 9/10\n",
      "4073/4073 [==============================] - 9s 2ms/step - loss: 0.4566 - accuracy: 0.8032 - val_loss: 0.5151 - val_accuracy: 0.7984\n",
      "Epoch 10/10\n",
      "4073/4073 [==============================] - 9s 2ms/step - loss: 0.4519 - accuracy: 0.8027 - val_loss: 0.5030 - val_accuracy: 0.7992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14431ad30>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting model on training data\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/347 [==============================] - 0s 872us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46038840938713776, 0.8051872849464417]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "results = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a threshold for prediction\n",
    "results_df = pd.DataFrame()\n",
    "results_mask = results > 0.1\n",
    "for i in range(len(label_list)):\n",
    "    results_df[label_list[i]] = results_mask[:,i]\n",
    "    results_df[label_list[i]] = results_df[label_list[i]].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['sum']  = 0\n",
    "for key in label_list:\n",
    "    results_df['sum'] = results_df['sum'] + results_df[key]\n",
    "(results_df['sum'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fittedtailor</th>\n",
       "      <th>oversized</th>\n",
       "      <th>relaxed</th>\n",
       "      <th>semifit</th>\n",
       "      <th>straightregular</th>\n",
       "      <th>attribute_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fittedtailor</td>\n",
       "      <td></td>\n",
       "      <td>relaxed</td>\n",
       "      <td>semifit</td>\n",
       "      <td>straightregular</td>\n",
       "      <td>fittedtailor, relaxed, semifit, straightregular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fittedtailor</td>\n",
       "      <td>oversized</td>\n",
       "      <td>relaxed</td>\n",
       "      <td>semifit</td>\n",
       "      <td>straightregular</td>\n",
       "      <td>fittedtailor, oversized, relaxed, semifit, str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fittedtailor</td>\n",
       "      <td>oversized</td>\n",
       "      <td>relaxed</td>\n",
       "      <td>semifit</td>\n",
       "      <td></td>\n",
       "      <td>fittedtailor, oversized, relaxed, semifit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fittedtailor</td>\n",
       "      <td></td>\n",
       "      <td>relaxed</td>\n",
       "      <td>semifit</td>\n",
       "      <td>straightregular</td>\n",
       "      <td>fittedtailor, relaxed, semifit, straightregular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fittedtailor</td>\n",
       "      <td>oversized</td>\n",
       "      <td>relaxed</td>\n",
       "      <td>semifit</td>\n",
       "      <td>straightregular</td>\n",
       "      <td>fittedtailor, oversized, relaxed, semifit, str...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fittedtailor  oversized  relaxed  semifit  straightregular  \\\n",
       "0  fittedtailor             relaxed  semifit  straightregular   \n",
       "1  fittedtailor  oversized  relaxed  semifit  straightregular   \n",
       "2  fittedtailor  oversized  relaxed  semifit                    \n",
       "3  fittedtailor             relaxed  semifit  straightregular   \n",
       "4  fittedtailor  oversized  relaxed  semifit  straightregular   \n",
       "\n",
       "                                     attribute_value  \n",
       "0    fittedtailor, relaxed, semifit, straightregular  \n",
       "1  fittedtailor, oversized, relaxed, semifit, str...  \n",
       "2          fittedtailor, oversized, relaxed, semifit  \n",
       "3    fittedtailor, relaxed, semifit, straightregular  \n",
       "4  fittedtailor, oversized, relaxed, semifit, str...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = results_df.drop(columns = ['sum'])\n",
    "for key in label_list:\n",
    "    results_df[key] = results_df[key].apply(lambda x: key if x == 1 else '')\n",
    "\n",
    "results_df['attribute_value'] = ''\n",
    "for key in label_list:\n",
    "    results_df['attribute_value'] = results_df['attribute_value'] + ' ' + results_df[key]\n",
    "\n",
    "results_df['attribute_value'] = results_df['attribute_value'].apply(lambda x: ', '.join(x.split()))\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fittedtailor, relaxed, semifit, straightregular               1441\n",
       "fittedtailor, oversized, relaxed, semifit, straightregular    1145\n",
       "relaxed, semifit, straightregular                              352\n",
       "fittedtailor, oversized, relaxed, semifit                      286\n",
       "fittedtailor, oversized, semifit                                84\n",
       "oversized, relaxed, semifit, straightregular                    62\n",
       "fittedtailor, relaxed, semifit                                  52\n",
       "fittedtailor, semifit                                           31\n",
       "relaxed, semifit                                                10\n",
       "oversized, relaxed, semifit                                      1\n",
       "Name: attribute_value, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['attribute_value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import spacy\n",
    "import pickle\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('model.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fit(brand, product_full_name, description, details, brand_category):\n",
    "    def clean_text(x):\n",
    "        try:\n",
    "            x = re.sub(r'<.*?>', '',x)\n",
    "            x = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", x)\n",
    "            x = re.sub(r\"\\'s\", \" \\'s\", x)\n",
    "            x = re.sub(r\"\\'ve\", \" \\'ve\", x)\n",
    "            x = re.sub(r\"n\\'t\", \" n\\'t\", x)\n",
    "            x = re.sub(r\"\\'re\", \" \\'re\", x)\n",
    "            x = re.sub(r\"\\'d\", \" \\'d\", x)\n",
    "            x = re.sub(r\"\\'ll\", \" \\'ll\", x)\n",
    "            x = re.sub(r\",\", \" , \", x)\n",
    "            x = re.sub(r\"!\", \" ! \", x)\n",
    "            x = re.sub(r\"\\(\", \"\", x)\n",
    "            x = re.sub(r\"\\)\", \"\", x)\n",
    "            x = re.sub(r\"\\?\", \"\", x)\n",
    "            x = re.sub(r\"/\", \"\", x)\n",
    "            x = re.sub(r\"\\s{2,}\", \" \", x)\n",
    "            return x.lower()\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def lemmatizer(x):\n",
    "        return ' '.join([token.lemma_ for token in nlp(x)])\n",
    "        \n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    with open('model.pickle', 'rb') as handle:\n",
    "        model = pickle.load(handle)\n",
    "\n",
    "    data = pd.DataFrame({'brand':brand,'product_full_name':product_full_name,'description':description,'details':details,'brand_category':brand_category},index=[0]) \n",
    "    df = data.copy()\n",
    "    for key in df.columns:\n",
    "        df[key] = df[key].apply(clean_text)\n",
    "        df[key] = df[key].apply(remove_stopwords)\n",
    "        df[key] = df[key].apply(lemmatizer)\n",
    "        df[key] = df[key].apply(clean_text)\n",
    "        df[key] = df[key].apply(remove_stopwords)\n",
    "    df['text'] = (df['brand'] + ' ' + df['product_full_name'] + ' ' + df['description'] + ' ' + df['brand_category'] + ' ' + df['details']).apply(str)\n",
    "    X = df['text'].values\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    results = model.predict(X)\n",
    "    results_df = pd.DataFrame()\n",
    "    results_mask = results > 0.1\n",
    "    for i in range(len(label_list)):\n",
    "        results_df[label_list[i]] = results_mask[:,i]\n",
    "        results_df[label_list[i]] = results_df[label_list[i]].apply(int)\n",
    "    results_df['sum']  = 0\n",
    "    for key in label_list:\n",
    "        results_df['sum'] = results_df['sum'] + results_df[key]\n",
    "    (results_df['sum'] == 0).sum()\n",
    "    results_df = results_df.drop(columns = ['sum'])\n",
    "    for key in label_list:\n",
    "        results_df[key] = results_df[key].apply(lambda x: key if x == 1 else '')\n",
    "\n",
    "    results_df['attribute_value'] = ''\n",
    "    for key in label_list:\n",
    "        results_df['attribute_value'] = results_df['attribute_value'] + ' ' + results_df[key]\n",
    "\n",
    "    results_df['attribute_value'] = results_df['attribute_value'].apply(lambda x: ', '.join(x.split()))\n",
    "    data['attribute_value'] = results_df['attribute_value']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = \"frame\"\n",
    "product_full_name = \"les second medium noir\"\n",
    "description = \"'minimal , modern styling meet refined luxury les second caba tote craft exquisite leather , structured handbag equip adjustable high polish peg buttonhole double drop handle , center welt seam , detachable pouch , frame logo discreetly emboss'\"\n",
    "details = np.nan\n",
    "brand_category = 'accessory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>product_full_name</th>\n",
       "      <th>description</th>\n",
       "      <th>details</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>attribute_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>frame</td>\n",
       "      <td>les second medium noir</td>\n",
       "      <td>'minimal , modern styling meet refined luxury ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accessory</td>\n",
       "      <td>fittedtailor, oversized, relaxed, semifit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   brand       product_full_name  \\\n",
       "0  frame  les second medium noir   \n",
       "\n",
       "                                         description  details brand_category  \\\n",
       "0  'minimal , modern styling meet refined luxury ...      NaN      accessory   \n",
       "\n",
       "                             attribute_value  \n",
       "0  fittedtailor, oversized, relaxed, semifit  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fit(brand, product_full_name, description, details, brand_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame(results)\n",
    "results.to_excel('results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel('results_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMq1No2sP6lAklcpKteWhVR",
   "include_colab_link": true,
   "name": "Model_V5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
