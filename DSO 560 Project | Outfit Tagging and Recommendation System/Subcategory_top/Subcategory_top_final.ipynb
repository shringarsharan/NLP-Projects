{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Sizing_final.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/AishJo/Repository1/blob/master/LSTM_Glove_Category.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"buklJs6mdj5T","outputId":"93a70b90-68c8-4ff5-998c-59744baacb6a","executionInfo":{"status":"ok","timestamp":1588663036248,"user_tz":420,"elapsed":22766,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vvLHOmMukGGN","colab":{}},"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4Pxdph9JB-dG","colab":{}},"source":["#!pip install -U imbalanced-learn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OZMWoVO4kknG","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model, Sequential \n","from keras.layers import Input, Masking\n","from keras.layers import Dense, Input, Reshape\n","from keras.layers import Dropout, Activation\n","from keras.layers import Dense, GlobalAveragePooling1D, Activation\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import LSTM, SimpleRNN\n","from keras.layers import Embedding\n","import keras.backend as K \n","from imblearn.combine import SMOTETomek\n","from sklearn.model_selection import train_test_split\n","from keras.optimizers import Adam\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.merge import concatenate\n","from keras.layers.convolutional import MaxPooling1D\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from gensim.models.phrases import Phraser, Phrases\n","import re \n","from gensim.parsing.preprocessing import remove_stopwords\n","import spacy\n","nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1J88vxQgkvZY","outputId":"7c726433-2981-494a-cdf7-c851d3afd556","executionInfo":{"status":"ok","timestamp":1588663045823,"user_tz":420,"elapsed":3475,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%%time\n","# reading the data\n","df = pd.read_csv('/content/drive/Shared drives/DSO 560 NLP Project/data.csv')\n","# creating a subset of the relevant attribute name and then dropping the column\n","df = df[df['attribute_name'] == 'subcategorytop'].drop(columns = ['attribute_name'])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["CPU times: user 484 ms, sys: 52.6 ms, total: 537 ms\n","Wall time: 2.23 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B2aP4F-isl36","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":103},"outputId":"743c2f75-ac26-416b-8900-4a95dbbd47fa","executionInfo":{"status":"ok","timestamp":1588663064300,"user_tz":420,"elapsed":1324,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}}},"source":["df['attribute_value'].unique()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['tee', 'blouse', 'buttondown', 'camisole', 'peplum', 'mockneck',\n","       'active', 'tank', 'wrap', 'puffsleeve', 'turtleneck', 'knit',\n","       'croptop', 'vest', 'tunic', 'bodysuit', 'sportsbra', 'cardigan',\n","       'laceup', 'duster', 'shell', 'henley', 'bustier', 'polo', 'poncho'],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vyu18_culshy","outputId":"625dbbb9-b5dc-4e5d-b1b5-f9689256bcb2","executionInfo":{"status":"ok","timestamp":1588663125777,"user_tz":420,"elapsed":951,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}},"colab":{"base_uri":"https://localhost:8080/","height":728}},"source":["df.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>product_id</th>\n","      <th>brand</th>\n","      <th>product_full_name</th>\n","      <th>description</th>\n","      <th>brand_category</th>\n","      <th>attribute_value</th>\n","      <th>details</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6427</th>\n","      <td>01E2P1MQS0GQ4SE93SY68ZYMCC</td>\n","      <td>nike</td>\n","      <td>embroidered logo t shirt</td>\n","      <td>embroidered l pretty daisy ' 70 inspire touch ...</td>\n","      <td>unknown</td>\n","      <td>tee</td>\n","      <td>xxs 00 , xs 0 2 , s 4 6 , m 8 10 , l 12 14 , x...</td>\n","    </tr>\n","    <tr>\n","      <th>6516</th>\n","      <td>01E4RVPK7JS9977A2E2VTN2PTW</td>\n","      <td>nili lotan</td>\n","      <td>ceyda</td>\n","      <td>halter design elegant , slim fit feature wrap ...</td>\n","      <td>NaN</td>\n","      <td>blouse</td>\n","      <td>model 5'10 wear size small size small measure ...</td>\n","    </tr>\n","    <tr>\n","      <th>6526</th>\n","      <td>01E4RV59Q8EQ0B5C14TKQZ3Z2V</td>\n","      <td>nili lotan</td>\n","      <td>brady tee</td>\n","      <td>dark navy , stripe t shirt distress soft cotto...</td>\n","      <td>NaN</td>\n","      <td>tee</td>\n","      <td>model 5'10 wear size small size small measure ...</td>\n","    </tr>\n","    <tr>\n","      <th>6547</th>\n","      <td>01E4EJ7KFN60HBT6GRCBA0Y44P</td>\n","      <td>anine bing</td>\n","      <td>luke shirt</td>\n","      <td>cut like timeless denim shirt , craft 100 leat...</td>\n","      <td>top</td>\n","      <td>buttondown</td>\n","      <td>style feature straight , slightly relaxed fit ...</td>\n","    </tr>\n","    <tr>\n","      <th>6576</th>\n","      <td>01E4VD397QRG2D91N2QMHKX7E4</td>\n","      <td>nanushka</td>\n","      <td>ella</td>\n","      <td>ella shirt oversize , relaxed fit crisp white ...</td>\n","      <td>shirt</td>\n","      <td>buttondown</td>\n","      <td>oversized camp shirt white bleach tumble dry i...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      product_id  ...                                            details\n","6427  01E2P1MQS0GQ4SE93SY68ZYMCC  ...  xxs 00 , xs 0 2 , s 4 6 , m 8 10 , l 12 14 , x...\n","6516  01E4RVPK7JS9977A2E2VTN2PTW  ...  model 5'10 wear size small size small measure ...\n","6526  01E4RV59Q8EQ0B5C14TKQZ3Z2V  ...  model 5'10 wear size small size small measure ...\n","6547  01E4EJ7KFN60HBT6GRCBA0Y44P  ...  style feature straight , slightly relaxed fit ...\n","6576  01E4VD397QRG2D91N2QMHKX7E4  ...  oversized camp shirt white bleach tumble dry i...\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4KCfa3CVn9nY","colab":{}},"source":["# creating a feature combining brand, productname, description and brand category\n","df['text'] = (df['brand'] + ' ' + df['product_full_name'] + ' ' + df['description'] + ' ' + df['brand_category']).apply(str)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WMi2ULgelhFo","colab":{}},"source":["# the predictor 'text' is assigned to 'X' and the 'attribute_value' t 'y'\n","X = df['text'].values\n","# one-hot-encoding the y variable\n","y = pd.get_dummies(df['attribute_value'])\n","label_list = y.columns\n","y = y.values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0YFmF01NwQGo","colab":{}},"source":["# creating a function to create uni-grams and tokenizing data\n","def encode_1gram(X, mode = 'binary'):\n","    tokenizer = Tokenizer(num_words=500)\n","    tokenizer.fit_on_texts(X)\n","    length = max([len(s.split()) for s in df['text']])\n","    X = tokenizer.texts_to_matrix(X, mode)\n","    X = pad_sequences(X, maxlen=length, padding='post')\n","    vocab_size = len(tokenizer.word_index) + 1\n","    return X, length, vocab_size"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5oETme721eAP","colab":{}},"source":["# creating a function to create bi-grams and tokenizing data\n","def encode_2gram(X, mode='binary'):\n","    phrases = Phrases(X, min_count=30)\n","    bigrams = Phraser(phrases)\n","    X = list(bigrams[X])\n","\n","    tokenizer = Tokenizer(num_words=500)\n","    tokenizer.fit_on_texts(X)\n","    length = max([len(s.split()) for s in df['text']])\n","    X = tokenizer.texts_to_matrix(X, mode)\n","    X = pad_sequences(X, maxlen=length, padding='post')\n","    vocab_size = len(tokenizer.word_index) + 1\n","    return X, length, vocab_size"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"30eS_YrlwyL9","colab":{}},"source":["#Tokenizer.texts_to_matrix()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6oEaE9mamM7X","colab":{}},"source":["# creating tf-idf vectors from uni-gram encoding\n","X1, length1, vocab_size1 = encode_1gram(X, mode = 'tfidf')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YGFzNuUUxWmZ","colab":{}},"source":["# creating tf-idf vectors from bi-gram encoding\n","X2, length2, vocab_size2 = encode_2gram(X, mode = 'tfidf')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JM7aXsN--SXZ","colab":{}},"source":["#X1 = X1.reshape(-1, 155, 1)\n","#X2 = X2.reshape(-1, 155, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1588663126601,"user_tz":420,"elapsed":1726,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}},"id":"hJpzT_ygA7Pw","outputId":"749b7aa1-79ef-4270-94e1-7b2ff91bf6b3","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X1.shape"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1077, 127)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yf8Gl0NGA1HV","colab":{}},"source":["# concatenating uni-gram and bi-gram\n","X = np.concatenate([X1, X2], axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ae88zjZ9CZye","colab":{}},"source":["# splitting data into training and test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhZb55qXr-_o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":487},"outputId":"45cf6394-d7bd-416d-d3f8-a6eac4c0206d","executionInfo":{"status":"error","timestamp":1588663127170,"user_tz":420,"elapsed":2284,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}}},"source":["# balancing the sample sets as the proportion of labels is unbalanced\n","resampler = SMOTETomek(sampling_strategy = 'auto')\n","X_train, y_train = resampler.fit_resample(X_train, y_train)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-e2c20c87abfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     83\u001b[0m             self.sampling_strategy, y, self._sampling_type)\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imblearn/combine/_smote_tomek.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_strategy_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmote_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtomek_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     83\u001b[0m             self.sampling_strategy, y, self._sampling_type)\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imblearn/over_sampling/_smote.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imblearn/over_sampling/_smote.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             \u001b[0mnns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m             X_new, y_new = self._make_samples(X_class, y.dtype, class_sample,\n\u001b[1;32m    814\u001b[0m                                               X_class, nns, n_samples, 1.0)\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 \u001b[0;34m\"Expected n_neighbors <= n_samples, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0;34m\" but n_samples = %d, n_neighbors = %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mn_samples_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m             )\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 1, n_neighbors = 6"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XNTeR6ZRoxcL","colab":{}},"source":["num_classes = y.shape[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-xgZ_gpen5wR","colab":{}},"source":["# defining LSTM model with sigmoid activation functions for prediction\n","def define_model():\n","    inputs = Input(shape=(length1+length2,))\n","    #x1 = Conv1D(filters=num_classes, kernel_size=1, padding='valid')(inputs1)\n","    embedding = Embedding(input_dim=vocab_size1, output_dim=100)(inputs)\n","\n","    #inputs2 = Input(shape=(length2,))\n","    #x2 = Conv1D(filters=num_classes, kernel_size=1, padding='valid')(inputs2)\n","    #dense2 = Dense(100, activation='relu')(inputs2)\n","    #embedding2 = Embedding(input_dim=vocab_size2, output_dim=100)(inputs2)\n","\n","    #merged = concatenate([inputs1, inputs1])\n","    x = LSTM(16, return_sequences=True, dropout=0.2, recurrent_dropout=0.15)(embedding)\n","\n","    #x = Dense(100, activation = 'relu')(inputs)\n","\n","\n","    x = Conv1D(filters=num_classes, kernel_size=1, padding='valid')(x)\n","    x = Conv1D(filters=num_classes, kernel_size=length1+length2, padding='valid')(x)\n","    x = Reshape((num_classes,))(x)\n","    #x = Dense(num_classes)(x)\n","    out = Activation('sigmoid')(x)\n","\n","    model = Model(inputs = [inputs], outputs = out)\n","\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1588663239860,"user_tz":420,"elapsed":16681,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}},"id":"WtVkDeU4pOFw","outputId":"5806db5d-0498-442f-8fe7-7fa09424c8a6","colab":{"base_uri":"https://localhost:8080/","height":398}},"source":["model = define_model()\n","model.summary()"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 254)               0         \n","_________________________________________________________________\n","embedding_1 (Embedding)      (None, 254, 100)          250900    \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 254, 16)           7488      \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 254, 25)           425       \n","_________________________________________________________________\n","conv1d_2 (Conv1D)            (None, 1, 25)             158775    \n","_________________________________________________________________\n","reshape_1 (Reshape)          (None, 25)                0         \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 25)                0         \n","=================================================================\n","Total params: 417,588\n","Trainable params: 417,588\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1588663264819,"user_tz":420,"elapsed":41634,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}},"id":"wfeN4H1XpRvM","outputId":"f7f2eee2-98d2-4ee1-c159-04a1c51e9b08","colab":{"base_uri":"https://localhost:8080/","height":453}},"source":["# fitting data onto model\n","model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=512)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 775 samples, validate on 194 samples\n","Epoch 1/10\n","775/775 [==============================] - 9s 12ms/step - loss: 0.6601 - accuracy: 0.7381 - val_loss: 0.5086 - val_accuracy: 0.9600\n","Epoch 2/10\n","775/775 [==============================] - 2s 2ms/step - loss: 0.4729 - accuracy: 0.9600 - val_loss: 0.2974 - val_accuracy: 0.9600\n","Epoch 3/10\n","775/775 [==============================] - 2s 2ms/step - loss: 0.2727 - accuracy: 0.9600 - val_loss: 0.1662 - val_accuracy: 0.9600\n","Epoch 4/10\n","775/775 [==============================] - 2s 2ms/step - loss: 0.1593 - accuracy: 0.9600 - val_loss: 0.1382 - val_accuracy: 0.9600\n","Epoch 5/10\n","775/775 [==============================] - 2s 2ms/step - loss: 0.1369 - accuracy: 0.9600 - val_loss: 0.1442 - val_accuracy: 0.9600\n","Epoch 6/10\n","775/775 [==============================] - 2s 2ms/step - loss: 0.1414 - accuracy: 0.9600 - val_loss: 0.1483 - val_accuracy: 0.9600\n","Epoch 7/10\n","775/775 [==============================] - 2s 2ms/step - loss: 0.1439 - accuracy: 0.9600 - val_loss: 0.1496 - val_accuracy: 0.9600\n","Epoch 8/10\n","775/775 [==============================] - 2s 2ms/step - loss: 0.1487 - accuracy: 0.9600 - val_loss: 0.1560 - val_accuracy: 0.9600\n","Epoch 9/10\n","775/775 [==============================] - 2s 2ms/step - loss: 0.1541 - accuracy: 0.9600 - val_loss: 0.1517 - val_accuracy: 0.9600\n","Epoch 10/10\n","775/775 [==============================] - 2s 2ms/step - loss: 0.1482 - accuracy: 0.9600 - val_loss: 0.1445 - val_accuracy: 0.9600\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fae76d25320>"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1588663265059,"user_tz":420,"elapsed":41867,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}},"id":"RH0rgY1WChE4","outputId":"f2dbc1fc-b00f-471c-c167-3c41948dca74","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# evaluating model\n","model.evaluate(X_test, y_test)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["108/108 [==============================] - 0s 4ms/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.1310218682995549, 0.9599999189376831]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"58YGb44Ln1sL","colab":{}},"source":["# probability of predicting each class for various records\n","results = model.predict(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C6FgcHherHja","colab":{}},"source":["#Creating a DataFrame for the results\n","results_df = pd.DataFrame()\n","results_mask = results > 0.1\n","for i in range(len(label_list)):\n","    results_df[label_list[i]] = results_mask[:,i]\n","    results_df[label_list[i]] = results_df[label_list[i]].apply(int)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1588663267172,"user_tz":420,"elapsed":43957,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}},"id":"YpNDlwRQsFj5","outputId":"5ff76c90-c649-45ab-9cc0-de0cd3f31a3f","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["results_df['sum']  = 0\n","for key in label_list:\n","    results_df['sum'] = results_df['sum'] + results_df[key]\n","(results_df['sum'] == 0).sum()"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1588663267173,"user_tz":420,"elapsed":43950,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}},"id":"5WsQqrmxprsc","outputId":"1c5561d4-fec5-4027-d8cb-d9523648d6b2","colab":{"base_uri":"https://localhost:8080/","height":312}},"source":["results_df = results_df.drop(columns = ['sum'])\n","for key in label_list:\n","    results_df[key] = results_df[key].apply(lambda x: key if x == 1 else '')\n","\n","results_df['attribute_value'] = ''\n","for key in label_list:\n","    results_df['attribute_value'] = results_df['attribute_value'] + ' ' + results_df[key]\n","\n","results_df['attribute_value'] = results_df['attribute_value'].apply(lambda x: ', '.join(x.split()))\n","results_df.head()"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>active</th>\n","      <th>blouse</th>\n","      <th>bodysuit</th>\n","      <th>bustier</th>\n","      <th>buttondown</th>\n","      <th>camisole</th>\n","      <th>cardigan</th>\n","      <th>croptop</th>\n","      <th>duster</th>\n","      <th>henley</th>\n","      <th>knit</th>\n","      <th>laceup</th>\n","      <th>mockneck</th>\n","      <th>peplum</th>\n","      <th>polo</th>\n","      <th>poncho</th>\n","      <th>puffsleeve</th>\n","      <th>shell</th>\n","      <th>sportsbra</th>\n","      <th>tank</th>\n","      <th>tee</th>\n","      <th>tunic</th>\n","      <th>turtleneck</th>\n","      <th>vest</th>\n","      <th>wrap</th>\n","      <th>attribute_value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td></td>\n","      <td>blouse</td>\n","      <td></td>\n","      <td></td>\n","      <td>buttondown</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>tee</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>blouse, buttondown, tee</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td></td>\n","      <td>blouse</td>\n","      <td></td>\n","      <td></td>\n","      <td>buttondown</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>tee</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>blouse, buttondown, tee</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td></td>\n","      <td>blouse</td>\n","      <td></td>\n","      <td></td>\n","      <td>buttondown</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>tee</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>blouse, buttondown, tee</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td></td>\n","      <td>blouse</td>\n","      <td></td>\n","      <td></td>\n","      <td>buttondown</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>tee</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>blouse, buttondown, tee</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td></td>\n","      <td>blouse</td>\n","      <td></td>\n","      <td></td>\n","      <td>buttondown</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>tee</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>blouse, buttondown, tee</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  active  blouse bodysuit bustier  ... turtleneck vest wrap          attribute_value\n","0         blouse                   ...                       blouse, buttondown, tee\n","1         blouse                   ...                       blouse, buttondown, tee\n","2         blouse                   ...                       blouse, buttondown, tee\n","3         blouse                   ...                       blouse, buttondown, tee\n","4         blouse                   ...                       blouse, buttondown, tee\n","\n","[5 rows x 26 columns]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1588663267173,"user_tz":420,"elapsed":43943,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}},"id":"BmfNQQbrqhhc","outputId":"13092fa4-c332-4e2e-8f8c-cbf0d49a6a6a","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["results_df['attribute_value'].value_counts()"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["blouse, buttondown, tee    1077\n","Name: attribute_value, dtype: int64"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"Z4SDtuSjpKSH","colab_type":"code","colab":{}},"source":["# the predictor 'text' is assigned to 'X' and the 'attribute_value' t 'y'\n","X = df['text'].values\n","# one-hot-encoding the y variable\n","y = pd.get_dummies(df['attribute_value'])\n","label_list = y.columns\n","y = y.values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0IJ3MgDrpKSJ","colab":{}},"source":["# tokenizing the data and integer encoding it\n","tokenizer = Tokenizer(num_words=5000, oov_token=\"UNKNOWN_TOKEN\")\n","tokenizer.fit_on_texts(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XTtmlA9FpKSL","colab":{}},"source":["# running the tokenizer function on features\n","X = tokenizer.texts_to_sequences(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_IMFmGLi6sUH","colab":{}},"source":["# getting vocab size and length for input\n","length = max([len(s.split()) for s in df['text']])\n","vocab_size = len(tokenizer.word_index) + 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jHn0BxkM6zbH","colab":{}},"source":["# padding the data to make it same size (max size)\n","X = pad_sequences(X, maxlen=length, padding='post')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HCI7I1ChpKSV","colab":{}},"source":["# splitting training and testing data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KPtrZfyDpKSY","colab":{"base_uri":"https://localhost:8080/","height":487},"outputId":"aac5e037-c807-4f05-bdec-52dfbce7e10c","executionInfo":{"status":"error","timestamp":1588663450841,"user_tz":420,"elapsed":668,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}}},"source":["# balancing the dataset to account for disproportionate of labels\n","resampler = SMOTETomek(sampling_strategy = 'auto')\n","X_train, y_train = resampler.fit_resample(X_train, y_train)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-e2c20c87abfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     83\u001b[0m             self.sampling_strategy, y, self._sampling_type)\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imblearn/combine/_smote_tomek.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_strategy_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmote_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtomek_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     83\u001b[0m             self.sampling_strategy, y, self._sampling_type)\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imblearn/over_sampling/_smote.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imblearn/over_sampling/_smote.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             \u001b[0mnns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m             X_new, y_new = self._make_samples(X_class, y.dtype, class_sample,\n\u001b[1;32m    814\u001b[0m                                               X_class, nns, n_samples, 1.0)\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 \u001b[0;34m\"Expected n_neighbors <= n_samples, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0;34m\" but n_samples = %d, n_neighbors = %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mn_samples_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m             )\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 1, n_neighbors = 6"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wZrKT65ipKSb","colab":{}},"source":["num_classes = y.shape[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8F-6edne7CKl","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bec8a2b1-c2d9-49e0-b62e-d49f002e242a","executionInfo":{"status":"ok","timestamp":1588663466818,"user_tz":420,"elapsed":15763,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}}},"source":["# using glove vector to create function that makes word emebeddings\n","def load_glove_vectors():\n","    embeddings_index = {}\n","    with open('/content/drive/Shared drives/DSO 560 NLP Project/glove.6B.100d.txt') as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","    print('Loaded %s word vectors.' % len(embeddings_index))\n","    return embeddings_index\n","\n","\n","embeddings_index = load_glove_vectors()"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Loaded 400000 word vectors.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Cm2saCqq7B6t","colab":{}},"source":["# create a weight matrix for words in training docs\n","embedding_matrix = np.zeros((vocab_size, 100))\n","for word, i in tokenizer.word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n","        embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GcWVkLPD7Bq6","colab":{}},"source":["# creating an LSTM model\n","def define_model():\n","    \n","    model = Sequential()\n","    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=length, trainable=False))\n","    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n","    model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15, activation = 'relu'))\n","    model.add(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.15, activation = 'relu'))\n","    #model.add(Flatten())\n","    model.add(Dense(16, activation = 'relu'))\n","    model.add(Dense(num_classes, activation='sigmoid'))\n","    \n","    # Compile the model\n","    model.compile(\n","    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RwR86N_D8RPY"},"source":["def define_model():\n","    \n","    model = Sequential()\n","    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=length, trainable=False))\n","    #model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n","    LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.15, activation = 'relu')\n","    LSTM(128, return_sequences=False, dropout=0.2, recurrent_dropout=0.15, activation = 'relu')\n","    model.add(Flatten())\n","    model.add(Dense(16, activation = 'relu'))\n","    model.add(Dense(num_classes, activation='sigmoid'))\n","    \n","    # Compile the model\n","    model.compile(\n","    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    \n","    return model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mG2NiGEkpKSn","colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"d3c93b63-2e2f-4a52-d508-e9910b6c91b5","executionInfo":{"status":"ok","timestamp":1588663472427,"user_tz":420,"elapsed":977,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}}},"source":["model = define_model()\n","model.summary()"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 127, 100)          251000    \n","_________________________________________________________________\n","masking_1 (Masking)          (None, 127, 100)          0         \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 127, 64)           42240     \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 64)                33024     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 16)                1040      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 25)                425       \n","=================================================================\n","Total params: 327,729\n","Trainable params: 76,729\n","Non-trainable params: 251,000\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4W5rC8OmpKSq","colab":{"base_uri":"https://localhost:8080/","height":418},"outputId":"3bc60fa9-efda-45d1-8ec9-f08eab3762fd","executionInfo":{"status":"ok","timestamp":1588663497608,"user_tz":420,"elapsed":25709,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}}},"source":["# fitting model on training data\n","model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=512)"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Train on 775 samples, validate on 194 samples\n","Epoch 1/10\n","775/775 [==============================] - 3s 4ms/step - loss: 0.6913 - accuracy: 0.5503 - val_loss: 0.6843 - val_accuracy: 0.7082\n","Epoch 2/10\n","775/775 [==============================] - 2s 3ms/step - loss: 0.6819 - accuracy: 0.6902 - val_loss: 0.6725 - val_accuracy: 0.7410\n","Epoch 3/10\n","775/775 [==============================] - 2s 3ms/step - loss: 0.6678 - accuracy: 0.7354 - val_loss: 0.6529 - val_accuracy: 0.7538\n","Epoch 4/10\n","775/775 [==============================] - 2s 3ms/step - loss: 0.6448 - accuracy: 0.7512 - val_loss: 0.6184 - val_accuracy: 0.7596\n","Epoch 5/10\n","775/775 [==============================] - 2s 3ms/step - loss: 1.2939 - accuracy: 0.7564 - val_loss: 1.9784 - val_accuracy: 0.7619\n","Epoch 6/10\n","775/775 [==============================] - 2s 3ms/step - loss: 7.7327 - accuracy: 0.7610 - val_loss: 6.4722 - val_accuracy: 0.7699\n","Epoch 7/10\n","775/775 [==============================] - 2s 3ms/step - loss: 66.5951 - accuracy: 0.7717 - val_loss: 5.4628 - val_accuracy: 0.7792\n","Epoch 8/10\n","775/775 [==============================] - 2s 3ms/step - loss: 186.7889 - accuracy: 0.7745 - val_loss: 3.6490 - val_accuracy: 0.7852\n","Epoch 9/10\n","775/775 [==============================] - 2s 3ms/step - loss: 12.7565 - accuracy: 0.7778 - val_loss: 3.0387 - val_accuracy: 0.7858\n","Epoch 10/10\n","775/775 [==============================] - 2s 3ms/step - loss: 7.8238 - accuracy: 0.7811 - val_loss: 1.8850 - val_accuracy: 0.7901\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fae551e6d30>"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6OxdBzEypKSt","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"afa9db94-2357-474c-b83c-94cae0df6b70","executionInfo":{"status":"ok","timestamp":1588663498138,"user_tz":420,"elapsed":25828,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}}},"source":["# evaluating model\n","model.evaluate(X_test, y_test)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["108/108 [==============================] - 1s 5ms/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.5974860456254747, 0.7774074673652649]"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ythmDKKlpjsc","colab":{}},"source":["# making predictions\n","results = model.predict(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Xjn1GCho_snJ","colab":{}},"source":["# defining a threshold for prediction\n","results_df = pd.DataFrame()\n","results_mask = results > 0.5\n","for i in range(len(label_list)):\n","    results_df[label_list[i]] = results_mask[:,i]\n","    results_df[label_list[i]] = results_df[label_list[i]].apply(int)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-Xcli7AxpKS0","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0b0494c6-d213-441f-ac43-b4f9969923da","executionInfo":{"status":"ok","timestamp":1588663502653,"user_tz":420,"elapsed":29128,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}}},"source":["results_df['sum']  = 0\n","for key in label_list:\n","    results_df['sum'] = results_df['sum'] + results_df[key]\n","(results_df['sum'] == 0).sum()"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Boo1IX91pKS5","colab":{"base_uri":"https://localhost:8080/","height":330},"outputId":"3b887a1c-6fc9-41a1-d54b-ca1656f485de","executionInfo":{"status":"ok","timestamp":1588663502654,"user_tz":420,"elapsed":28635,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}}},"source":["results_df = results_df.drop(columns = ['sum'])\n","for key in label_list:\n","    results_df[key] = results_df[key].apply(lambda x: key if x == 1 else '')\n","\n","results_df['attribute_value'] = ''\n","for key in label_list:\n","    results_df['attribute_value'] = results_df['attribute_value'] + ' ' + results_df[key]\n","\n","results_df['attribute_value'] = results_df['attribute_value'].apply(lambda x: ', '.join(x.split()))\n","results_df.head()"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>active</th>\n","      <th>blouse</th>\n","      <th>bodysuit</th>\n","      <th>bustier</th>\n","      <th>buttondown</th>\n","      <th>camisole</th>\n","      <th>cardigan</th>\n","      <th>croptop</th>\n","      <th>duster</th>\n","      <th>henley</th>\n","      <th>knit</th>\n","      <th>laceup</th>\n","      <th>mockneck</th>\n","      <th>peplum</th>\n","      <th>polo</th>\n","      <th>poncho</th>\n","      <th>puffsleeve</th>\n","      <th>shell</th>\n","      <th>sportsbra</th>\n","      <th>tank</th>\n","      <th>tee</th>\n","      <th>tunic</th>\n","      <th>turtleneck</th>\n","      <th>vest</th>\n","      <th>wrap</th>\n","      <th>attribute_value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>bustier</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>croptop</td>\n","      <td>duster</td>\n","      <td></td>\n","      <td></td>\n","      <td>laceup</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>shell</td>\n","      <td></td>\n","      <td></td>\n","      <td>tee</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>bustier, croptop, duster, laceup, shell, tee</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>bustier</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>bustier</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>bustier</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>bustier</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>bustier</td>\n","      <td></td>\n","      <td>camisole</td>\n","      <td></td>\n","      <td>croptop</td>\n","      <td>duster</td>\n","      <td></td>\n","      <td></td>\n","      <td>laceup</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>shell</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>bustier, camisole, croptop, duster, laceup, shell</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>bustier</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>croptop</td>\n","      <td>duster</td>\n","      <td></td>\n","      <td></td>\n","      <td>laceup</td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td></td>\n","      <td>tunic</td>\n","      <td></td>\n","      <td>vest</td>\n","      <td></td>\n","      <td>bustier, croptop, duster, laceup, tunic, vest</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  active blouse  ... wrap                                    attribute_value\n","0                ...            bustier, croptop, duster, laceup, shell, tee\n","1                ...                                                 bustier\n","2                ...                                                 bustier\n","3                ...       bustier, camisole, croptop, duster, laceup, shell\n","4                ...           bustier, croptop, duster, laceup, tunic, vest\n","\n","[5 rows x 26 columns]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kBsLjXEzyvys","colab":{"base_uri":"https://localhost:8080/","height":623},"outputId":"204d7e61-4779-4e6e-e227-dd6b4f8419da","executionInfo":{"status":"ok","timestamp":1588663503008,"user_tz":420,"elapsed":28500,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}}},"source":["results_df['attribute_value'].value_counts()"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["bustier                                                                                         300\n","bustier, croptop, duster, laceup, shell                                                         226\n","bustier, croptop, duster, laceup, shell, tee                                                    197\n","bustier, camisole, croptop, duster, laceup, shell                                               188\n","bustier, camisole, croptop, duster, laceup, shell, tee                                           54\n","bustier, camisole, croptop, duster, laceup, vest                                                 28\n","bustier, camisole, croptop, duster, laceup                                                       24\n","bustier, duster, laceup, shell, tee                                                              15\n","bustier, camisole, croptop, duster, laceup, tunic, vest                                           6\n","bustier, buttondown, croptop, duster, laceup, tunic, vest                                         5\n","bustier, camisole, croptop, duster, laceup, tee, tunic, vest                                      4\n","bustier, buttondown, croptop, duster, knit, laceup, tunic, turtleneck                             2\n","blouse, bustier, buttondown, croptop, duster, knit, laceup, tee, tunic, turtleneck, vest          2\n","bustier, croptop, duster, laceup, tunic, vest                                                     2\n","bustier, buttondown, croptop, duster, laceup, puffsleeve, tee, turtleneck                         2\n","bustier, duster, laceup, shell                                                                    2\n","bustier, buttondown, croptop, duster, laceup, tunic, turtleneck, vest                             2\n","bodysuit, bustier, buttondown, duster, knit, shell, turtleneck                                    2\n","bustier, buttondown, croptop, duster, laceup, tunic, turtleneck                                   1\n","bustier, buttondown, croptop, duster, knit, tunic, turtleneck, vest                               1\n","bustier, buttondown, croptop, duster, laceup, poncho, puffsleeve, shell, turtleneck               1\n","bustier, buttondown, croptop, duster, knit, poncho, puffsleeve, tunic, turtleneck                 1\n","bustier, buttondown, croptop, duster, shell, tunic, vest                                          1\n","bustier, buttondown, camisole, croptop, duster, knit, laceup, peplum, tunic, vest                 1\n","bodysuit, bustier, croptop, duster, peplum, turtleneck, vest                                      1\n","bustier, camisole, croptop, duster, laceup, shell, vest                                           1\n","bodysuit, bustier, buttondown, duster, henley, knit, laceup, peplum, shell, turtleneck, vest      1\n","bustier, buttondown, duster, laceup, shell, tee                                                   1\n","bustier, camisole, croptop, duster, knit, peplum, tee, tunic, vest                                1\n","blouse, bustier, camisole, duster, laceup, shell, sportsbra, tee                                  1\n","blouse, bustier, buttondown, croptop, duster, knit, laceup, shell, tee, tunic, vest               1\n","bustier, buttondown, croptop, duster, laceup, tee, tunic, turtleneck                              1\n","bustier, croptop, duster, laceup, vest                                                            1\n","bustier, buttondown, croptop, duster, laceup, tee, tunic, turtleneck, vest                        1\n","Name: attribute_value, dtype: int64"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C8G0Q1SjfeQv","colab":{}},"source":["import pickle\n","with open('tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","with open('model.pickle', 'wb') as handle:\n","    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TZFAGD_chS6z","colab":{}},"source":["def get_subcategorytop(brand, product_full_name, description, details, brand_category):\n","    def clean_text(x):\n","        try:\n","            x = re.sub(r'<.*?>', '',x)\n","            x = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", x)\n","            x = re.sub(r\"\\'s\", \" \\'s\", x)\n","            x = re.sub(r\"\\'ve\", \" \\'ve\", x)\n","            x = re.sub(r\"n\\'t\", \" n\\'t\", x)\n","            x = re.sub(r\"\\'re\", \" \\'re\", x)\n","            x = re.sub(r\"\\'d\", \" \\'d\", x)\n","            x = re.sub(r\"\\'ll\", \" \\'ll\", x)\n","            x = re.sub(r\",\", \" , \", x)\n","            x = re.sub(r\"!\", \" ! \", x)\n","            x = re.sub(r\"\\(\", \"\", x)\n","            x = re.sub(r\"\\)\", \"\", x)\n","            x = re.sub(r\"\\?\", \"\", x)\n","            x = re.sub(r\"/\", \"\", x)\n","            x = re.sub(r\"\\s{2,}\", \" \", x)\n","            return x.lower()\n","        except:\n","            return ''\n","\n","    def lemmatizer(x):\n","        return ' '.join([token.lemma_ for token in nlp(x)])\n","        \n","    with open('tokenizer.pickle', 'rb') as handle:\n","        tokenizer = pickle.load(handle)\n","    with open('model.pickle', 'rb') as handle:\n","        model = pickle.load(handle)\n","\n","    data = pd.DataFrame({'brand':brand,'product_full_name':product_full_name,'description':description,'details':details,'brand_category':brand_category},index=[0]) \n","    df = data.copy()\n","    for key in df.columns:\n","        df[key] = df[key].apply(clean_text)\n","        df[key] = df[key].apply(remove_stopwords)\n","        df[key] = df[key].apply(lemmatizer)\n","        df[key] = df[key].apply(clean_text)\n","        df[key] = df[key].apply(remove_stopwords)\n","    df['text'] = (df['brand'] + ' ' + df['product_full_name'] + ' ' + df['description'] + ' ' + df['brand_category'] + ' ' + df['details']).apply(str)\n","    X = df['text'].values\n","    X = tokenizer.texts_to_sequences(X)\n","    X = pad_sequences(X, maxlen=length, padding='post')\n","    results = model.predict(X)\n","    results_df = pd.DataFrame()\n","    results_mask = results > 0.1\n","    for i in range(len(label_list)):\n","        results_df[label_list[i]] = results_mask[:,i]\n","        results_df[label_list[i]] = results_df[label_list[i]].apply(int)\n","    results_df['sum']  = 0\n","    for key in label_list:\n","        results_df['sum'] = results_df['sum'] + results_df[key]\n","    (results_df['sum'] == 0).sum()\n","    results_df = results_df.drop(columns = ['sum'])\n","    for key in label_list:\n","        results_df[key] = results_df[key].apply(lambda x: key if x == 1 else '')\n","\n","    results_df['attribute_value'] = ''\n","    for key in label_list:\n","        results_df['attribute_value'] = results_df['attribute_value'] + ' ' + results_df[key]\n","\n","    results_df['attribute_value'] = results_df['attribute_value'].apply(lambda x: ', '.join(x.split()))\n","    data['attribute_value'] = results_df['attribute_value']\n","    return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pHysGjOuhlea","colab":{}},"source":["brand = \"frame\"\n","product_full_name = \"les second medium noir\"\n","description = \"'minimal , modern styling meet refined luxury les second caba tote craft exquisite leather , structured handbag equip adjustable high polish peg buttonhole double drop handle , center welt seam , detachable pouch , frame logo discreetly emboss'\"\n","details = np.nan\n","brand_category = 'accessory'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Wq_DOyMbhrm4","colab":{"base_uri":"https://localhost:8080/","height":98},"outputId":"19ad9275-526c-406a-f929-91e0778aeb7c","executionInfo":{"status":"ok","timestamp":1588663572700,"user_tz":420,"elapsed":4174,"user":{"displayName":"Jasleen Kaur Ahuja","photoUrl":"","userId":"16363827072704495669"}}},"source":["get_subcategorytop(brand, product_full_name, description, details, brand_category)"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>brand</th>\n","      <th>product_full_name</th>\n","      <th>description</th>\n","      <th>details</th>\n","      <th>brand_category</th>\n","      <th>attribute_value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>frame</td>\n","      <td>les second medium noir</td>\n","      <td>'minimal , modern styling meet refined luxury ...</td>\n","      <td>NaN</td>\n","      <td>accessory</td>\n","      <td>active, blouse, bodysuit, bustier, buttondown,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   brand  ...                                    attribute_value\n","0  frame  ...  active, blouse, bodysuit, bustier, buttondown,...\n","\n","[1 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7DG5qSjShubb","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}